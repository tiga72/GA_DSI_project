{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 3: Web APIs & Classification\n",
    "---\n",
    "Project notebook organisation:<br>\n",
    "**1_Get_and_Clean_Data** (current notebook)<br>\n",
    "<a href='./2_EDA_and_Preprocessing.ipynb'>2_EDA_and_Preprocessing</a><br>\n",
    "<a href='./3_Models.ipynb'>3_Models</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### This notebook's layout\n",
    "<a href='#gld'>Scraping from Subreddit:Gold</a><br>\n",
    "<a href='#sil'>Scraping from Subreddit:Silverbugs</a><br>\n",
    "<a href='#import'>Re-import and and check dataframes</a><br>\n",
    "<a href='#duplicate'>Removes duplicates</a><br>\n",
    "<a href='#rename'>Column filtering and renaming</a><br>\n",
    "<a href='#miss'>Working on null/missing values</a><br>\n",
    "<a href='#comment'>Remove common comment headers</a><br>\n",
    "<a href='#rm_web'>Remove http and www</a><br>\n",
    "<a href='#function'>Data cleaning and processing in a pre-defined function</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem statement\n",
    "\n",
    "I am a financial quantitative analyst with a Hedge fund company. Recently, our company is re-balancing our portfolio on<br> precious metals and I was tasked to leverage on data to identify any trends, sentiments in precious metals sector.<br> \n",
    "\n",
    "The first step is to use a classification models to differentiate between a gold and silver related Reddit post.<br> \n",
    "This is critical before we proceed to the next step<br>\n",
    "\n",
    "Secondly, to examine Reddit's post and gaining insights to :\n",
    "\n",
    "Which precious metal is best for investment purposes?\n",
    "\n",
    "Which form of precious metals investment (CFDs, ETFs, futures, gold related stock, physical holdings)\n",
    "\n",
    "Any other findings from post related stats like the number of comments of likes to a title post\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from numpy.random import randint\n",
    "import numpy as np\n",
    "import emoji\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare urls and header variables\n",
    "url_gold = 'https://www.reddit.com/r/Gold.json'\n",
    "url_silver = 'https://www.reddit.com/r/Silverbugs.json'\n",
    "\n",
    "headers = {'User-agent':'my_agent007'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gld'></a>\n",
    "### Scraping from Subreddit:Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# This cell is commented-out\\n# Scraping takes time and the data is not the same when replicated\\n# Intent here is to show the code in getting the data\\ngld_posts = []\\nafter = None\\n \\nfor a in range(150):\\n    if after == None:\\n        current_url = url_gold\\n    else:\\n        current_url = url_gold + '?after=' + after\\n    res = requests.get(current_url, headers=headers)\\n    \\n    if res.status_code != 200:\\n        print('Status error', res.status_code)\\n        break\\n    \\n    current_dict = res.json()\\n    current_posts = [p['data'] for p in current_dict['data']['children']]\\n    gld_posts.extend(current_posts)\\n    after = current_dict['data']['after']\\n \\n    #  random sleep time\\n    time.sleep(randint(1,3,1))\\n\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# This cell is commented-out\n",
    "# Scraping takes time and the data is not the same when replicated\n",
    "# Intent here is to show the code in getting the data\n",
    "gld_posts = []\n",
    "after = None\n",
    " \n",
    "for a in range(150):\n",
    "    if after == None:\n",
    "        current_url = url_gold\n",
    "    else:\n",
    "        current_url = url_gold + '?after=' + after\n",
    "    res = requests.get(current_url, headers=headers)\n",
    "    \n",
    "    if res.status_code != 200:\n",
    "        print('Status error', res.status_code)\n",
    "        break\n",
    "    \n",
    "    current_dict = res.json()\n",
    "    current_posts = [p['data'] for p in current_dict['data']['children']]\n",
    "    gld_posts.extend(current_posts)\n",
    "    after = current_dict['data']['after']\n",
    " \n",
    "    #  random sleep time\n",
    "    time.sleep(randint(1,3,1))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sil'></a>\n",
    "### Scraping from Subreddit:Silverbugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# This cell is commented-out\\n# Scraping takes time and the data is not the same when replicated\\n# Intent here is to show the code in getting the data\\nsil_posts = []\\nafter = None\\n \\nfor a in range(150):\\n    if after == None:\\n        current_url = url_silver\\n    else:\\n        current_url = url_silver + '?after=' + after\\n    res = requests.get(current_url, headers=headers)\\n    \\n    if res.status_code != 200:\\n        print('Status error', res.status_code)\\n        break\\n    \\n    current_dict = res.json()\\n    current_posts = [p['data'] for p in current_dict['data']['children']]\\n    sil_posts.extend(current_posts)\\n    after = current_dict['data']['after']\\n \\n    #  random sleep time\\n    time.sleep(randint(1,3,1))\\n\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# This cell is commented-out\n",
    "# Scraping takes time and the data is not the same when replicated\n",
    "# Intent here is to show the code in getting the data\n",
    "sil_posts = []\n",
    "after = None\n",
    " \n",
    "for a in range(150):\n",
    "    if after == None:\n",
    "        current_url = url_silver\n",
    "    else:\n",
    "        current_url = url_silver + '?after=' + after\n",
    "    res = requests.get(current_url, headers=headers)\n",
    "    \n",
    "    if res.status_code != 200:\n",
    "        print('Status error', res.status_code)\n",
    "        break\n",
    "    \n",
    "    current_dict = res.json()\n",
    "    current_posts = [p['data'] for p in current_dict['data']['children']]\n",
    "    sil_posts.extend(current_posts)\n",
    "    after = current_dict['data']['after']\n",
    " \n",
    "    #  random sleep time\n",
    "    time.sleep(randint(1,3,1))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Checking the length after scrapping\\nprint(len(gld_posts))\\nprint(len(sil_posts))\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Checking the length after scrapping\n",
    "print(len(gld_posts))\n",
    "print(len(sil_posts))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Convert scrapped data into dataframe\\ngold = pd.DataFrame(gld_posts)\\nsilver = pd.DataFrame(sil_posts)\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Convert scrapped data into dataframe\n",
    "gold = pd.DataFrame(gld_posts)\n",
    "silver = pd.DataFrame(sil_posts)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(gold.shape)\\nprint(silver.shape)\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "print(gold.shape)\n",
    "print(silver.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Export data to csv\\ngold.to_csv('../data/gold.csv', index = False)\\nsilver.to_csv('../data/silver.csv', index = False)\\n\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Export data to csv\n",
    "gold.to_csv('../data/gold.csv', index = False)\n",
    "silver.to_csv('../data/silver.csv', index = False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='import'></a>\n",
    "### Re-import and and check dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import both subreddits\n",
    "gold = pd.read_csv('../data/gold.csv')\n",
    "silver = pd.read_csv('../data/silver.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6159, 110)\n",
      "(6223, 111)\n"
     ]
    }
   ],
   "source": [
    "# Checks the shape\n",
    "print(gold.shape)\n",
    "print(silver.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='duplicate'></a>\n",
    "### Removes duplicates\n",
    "\n",
    "Post are indexed via 'name' string variable<br>\n",
    "will need to remove duplicated ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     5284\n",
       "False     875\n",
       "Name: name, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking duplicates base on 'name'\n",
    "gold_duplicates_by_name = gold['name'].duplicated().value_counts()\n",
    "gold_duplicates_by_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     5182\n",
       "False    1041\n",
       "Name: name, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking duplicates base on 'name'\n",
    "silver_duplicates_by_name = silver['name'].duplicated().value_counts()\n",
    "silver_duplicates_by_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The inverse of duplicates are the non-duplicates in which we will keep \n",
    "gold = gold[~gold['name'].duplicated()]\n",
    "silver = silver[~silver['name'].duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(875, 110)\n",
      "(1041, 111)\n"
     ]
    }
   ],
   "source": [
    "# Checks to ensure the filtering is done correctly\n",
    "# 6159-5284 = 875\n",
    "# 6223-5182 = 1041\n",
    "print(gold.shape)\n",
    "print(silver.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold    875\n",
      "Name: subreddit, dtype: int64\n",
      "Silverbugs    1041\n",
      "Name: subreddit, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Further checks by looking at subreddit counts\n",
    "print(gold['subreddit'].value_counts())\n",
    "print(silver['subreddit'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rename'></a>\n",
    "### Column filtering and renaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 8 features from gold\n",
    "gold = gold[['name','author','title','selftext','permalink','num_comments','ups','subreddit']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 8 features from silver\n",
    "silver = silver[['name','author','title','selftext','permalink','num_comments','ups', 'subreddit']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some columns need to be rename for easier understanding\n",
    "new_column_name = ['post_id','author','title','post_body','comments','num_comments','ups', 'subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-assign column names\n",
    "gold.columns = new_column_name\n",
    "silver.columns = new_column_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['post_id', 'author', 'title', 'post_body', 'comments', 'num_comments',\n",
      "       'ups', 'subreddit'],\n",
      "      dtype='object')\n",
      "Index(['post_id', 'author', 'title', 'post_body', 'comments', 'num_comments',\n",
      "       'ups', 'subreddit'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check that the column re-naming is done correctly\n",
    "print(gold.columns)\n",
    "print(silver.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(875, 8)\n",
      "(1041, 8)\n"
     ]
    }
   ],
   "source": [
    "print(gold.shape)\n",
    "print(silver.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='miss'></a>\n",
    "### Working on null/missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post_body       443\n",
      "subreddit         0\n",
      "ups               0\n",
      "num_comments      0\n",
      "dtype: int64\n",
      "post_body       737\n",
      "subreddit         0\n",
      "ups               0\n",
      "num_comments      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking null values across all columns\n",
    "gold_isnull_val = gold.isnull().sum().sort_values(ascending = False)\n",
    "silver_isnull_val = silver.isnull().sum().sort_values(ascending = False)\n",
    "print(gold_isnull_val.head(4))\n",
    "print(silver_isnull_val.head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-assigns null values to 'None'\n",
    "#gold.loc[gold['post_body'].isnull(), 'post_body'] = 'None'\n",
    "#silver.loc[silver['post_body'].isnull(), 'post_body'] = 'None'\n",
    "\n",
    "gold['post_body'].fillna(value = 'None', inplace=True)\n",
    "silver['post_body'].fillna(value = 'None', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post_body    443\n",
      "subreddit      0\n",
      "dtype: int64\n",
      "post_body    737\n",
      "subreddit      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking that NaN replaced with 'None'\n",
    "print(gold_isnull_val.head(2))\n",
    "print(silver_isnull_val.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='comment'></a>\n",
    "### Remove common comment headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "875\n",
      "1041\n"
     ]
    }
   ],
   "source": [
    "# The standard header indicates the start of a comments in every post's comment\n",
    "# It is redundant and its data leaking in nature. It should be removed\n",
    "# \\w+\\d*\\w+|d*/ matches commentor id\n",
    "# regex explanation : (1 word one or more) AND (1 digit zero or more) AND (1 word one or more|1 digit zero or more)\n",
    "\n",
    "rm_gold_comment = '/r/Gold/comments/\\w+\\d*\\w+|d*/'\n",
    "rm_silver_comment = '/r/Silverbugs/comments/\\w+\\d*\\w+|d*/'\n",
    "\n",
    "# The number of 'comment header' is the same as the data's row\n",
    "g_count = gold['comments'].str.contains('/r/Gold/comments/').sum()\n",
    "s_count = silver['comments'].str.contains('/r/Silverbugs/comments/').sum()\n",
    "print(g_count)\n",
    "print(s_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the'comment header' in both subreddit\n",
    "gold['comments'] = gold['comments'].str.replace(rm_gold_comment, \"\")\n",
    "silver['comments'] = silver['comments'].str.replace(rm_silver_comment, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# checks that the removal was done correctly\n",
    "g_count = gold['comments'].str.contains('/r/Gold/comments/').sum()\n",
    "s_count = silver['comments'].str.contains('/r/Silverbugs/comments/').sum()\n",
    "\n",
    "print(g_count)\n",
    "print(s_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rm_web'></a>\n",
    "### Remove http and www"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_web(df):\n",
    "    \"\"\"Removes http,www on dataframe columns\"\"\"\n",
    "    common_column_names = ['title','post_body','comments']\n",
    "    for col_name in common_column_names:\n",
    "        df[col_name] = df[col_name].replace(r\"http\\S+\", '',regex=True)\n",
    "        df[col_name] = df[col_name].replace(r\"www\\S+\", '',regex=True)\n",
    "    print('http and www elements removed')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http and www elements removed\n",
      "http and www elements removed\n"
     ]
    }
   ],
   "source": [
    "rm_web(gold)\n",
    "rm_web(silver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='function'></a>\n",
    "### Data cleaning and processing in a pre-defined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(raw_review):\n",
    "    \"\"\" Performs many cleaning and processing in one function\"\"\"\n",
    "        \n",
    "    # - Remove HTML.\n",
    "    review_text = BeautifulSoup(raw_review).get_text()\n",
    "    \n",
    "    # - Remove non-letters.\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    \n",
    "    # - Remove whitespace (including new line characters)\n",
    "    text = re.sub(r'\\s\\s+', ' ', letters_only)\n",
    "   \n",
    "    # - Convert to lower case, split into individual words.\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    # - In Python, searching a set is much faster than searching\n",
    "    # a list, so convert the stopwords to a set.\n",
    "    stops = set(stopwords.words('english'))\n",
    "    \n",
    "    # - Remove stopwords.\n",
    "    stop_words = [w for w in words if not w in stops]\n",
    "    \n",
    "    # - Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    meaningful_words = [lemmatizer.lemmatize(w) for w in stop_words] \n",
    "  \n",
    "    # - Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return(\" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cleaner(df):\n",
    "    \"\"\"Run clean_data on dataframe columns\"\"\"\n",
    "    common_column_names = ['title','post_body','comments']\n",
    "    for col_name in common_column_names:\n",
    "        df[col_name] = df[col_name].apply(clean_data)\n",
    "    print('processing complete')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing complete\n"
     ]
    }
   ],
   "source": [
    "run_cleaner(gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing complete\n"
     ]
    }
   ],
   "source": [
    "run_cleaner(silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold.to_csv('../data/gold_clean.csv', index = False)\n",
    "silver.to_csv('../data/silver_clean.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
